{"name":"Homb","tagline":"Hybrid OpenMP MPI Benchmark","body":"Hybrid OpenMP MPI Benchmark\r\n========================================================================\r\n\r\nHOMB is the Hybrid OpenMP MPI Benchmark for testing hybrid codes on\r\nmulticore and SMP systems.  The code is an optimized Laplace Solver\r\non a 2-D grid (Point Jacobi) with optional convergence test.\r\n\r\nThe code is designed to compare the performance of MPI, OpenMP, and\r\nhybrid codes.  It will behave as a pure MPI code when the number of\r\nthreads per task is 1 and a pure OpenMP code when the number of \r\ntasks is 1.\r\n\r\nThere are two general tests corespoding to three types of output:\r\n\r\n*   Summary tests provide the statstical performance of HOMB\r\n*   Times tests provide the raw runtimes of each task\r\n\r\nTo understand how these tests can be used to draw conclusions, it \r\nis recommended that the user reads the code over or simply use\r\nsummary tests output (-s).  \r\n\r\nNOTE: all tests run the same code and only differ in output.\r\n\r\nJob Files\r\n------------------\r\n\r\nJob files that compile and run HOMB are avaliable on these machines:\r\n\r\n    PSC's Altix 4700 (Pople or Salk)\r\n    PSC's Xeon Cluster (Muir)\r\n    NCSA's Altix 3700 (Cobalt)\r\n    TACC's Sun Constellation Linux Cluster (Ranger)\r\n\r\nTo Build\r\n------------------\r\n\r\nIf there is a `Make.mach.<machine>` file for the desired machine:\r\n\r\n    make MACHINE=<machine>\r\n\r\nIf you want to add make support for your machine, copy a file for \r\nanother machine and change the options.\r\n\r\n--or--\r\n\r\nCompile with MPI and OpenMP support and desired optimization flags.\r\nExamples:\r\n\r\n    icc -o homb.ex -O3 -ftz -ipo -openmp -lmpi src/homb.c\r\n    gcc -o homb.ex -O3 -fopenmp -lmpi src/homb.c\r\n\r\nTo Run\r\n------------------\r\n\r\nRun with MPI/OpenMP support and following args:\r\n\r\n    -NC  val             Number of Columns in grid \r\n    -NR  val             Number of Rows in grid    \r\n    -NRC val             Number of Rows and Columns in grid \r\n    -NITER val           Number of iterations      \r\n    -[no_]barrier        Places MPI_Barrier at start of iterations\r\n    -[no_]reduce         Places MPI_Reduce at end of iterations\r\n    -s                   Summary to standard out\r\n    -v                   Verbose to standard out\r\n    -nh                  No header in standard out\r\n    -vf val              File name for verbose file output\r\n    -tf val              File name for times matrix file output\r\n    -pc                  Print Context (settings) to standard out\r\n\r\nExamples\r\n--------\r\n\r\n    mpirun -np 4 omplace -nt 4 ./homb.ex -NRC \\\r\n        -NITER 25 -vf homb-4-4-32768.out -s\r\n\r\nRun on 16 cores with 4 tasks and 4 threads per task over\r\n25 iterations with 512MB/core memory usage.  Outputs\r\nall data to file and a summary to Standard Out.\r\n\r\n    mpirun -np 4 ./homb.ex -NR 16384 -NC 16384 -NITER 10 -v\r\n\r\nRun on 4 cores with pure MPI over 10 iterations with\r\n512MB/core mem usage.  Outputs all data to stdout.\r\n\r\nUtilities\r\n---------\r\n\r\n#### [times_to_hist.py](utils/times_to_hist.py)\r\n\r\nThis script takes a times output file from HOMB and produces a\r\nhistogram of the runtimes, allowing the user to analyze the \r\ndistribution/variablility of the performance of the machine.\r\n\r\n#### [times_to_chain.py](utils/times_to_chain.py)\r\n\r\nThis script takes a times output file from HOMB and produces an\r\nanimation of exclusive runtimes of the individual PEs through time, \r\nallowing the user to see the relationships/coupling of the PEs.\r\nThe ouput can be thought of as the PE's chained together oscillating.\r\nFor example, on can deduce the general form of MPI_Reduce. \r\n\r\n#### [times_to_race.py](utils/times_to_race.py)\r\n\r\nThis script takes a times output file from HOMB and produces an\r\nanimation of inclusive runtimes of the individual PEs through time, \r\nallowing the user to see the relationships/coupling of the PEs.\r\nThe output can be thought of as the PE's racing (but low is good).\r\nFor example, on can deduce the general form of MPI_Reduce. \r\n\r\n#### [times_to_covar.py](utils/times_to_covar.py)\r\n\r\nThis script takes a times output file from HOMB and produces a\r\nmatrix of the covariances of ranks to one another.  This is useful\r\nfor finding the relative coupling of tasks.\r\n\r\n#### [sweep_to_graph.py](utils/sweep_to_graph.py)\r\n\r\nThis script takes a set of summary lines form multiple mulit-core \r\nruns and one single core run (of the same problem size) to plot\r\npeak, average, and minimum performance of various MPI/OpenMP ratios\r\n\r\n#### [sweeps_to_scales.py](utils/sweeps_to_scales.py)\r\n\r\nThis script takes a set of summary lines form multiple mulit-core \r\non multiple number of total cores (sweeps) and plots the peak, mean,\r\nand worst performance to show weak-scaling.\r\n\r\nMore info\r\n---------\r\n\r\nMax Hutchinson (mhutchin@psc.edu).\r\n\r\nCreator: Max Hutchinson (mhutchin@psc.edu)\r\n\r\nProto-code written by: Uriel Jaroslawski, Jon Iturralde\r\n","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}