<!DOCTYPE html>
<html>

  <head>
    <meta charset='utf-8' />
    <meta http-equiv="X-UA-Compatible" content="chrome=1" />
    <meta name="description" content="Homb : Hybrid OpenMP MPI Benchmark" />

    <link rel="stylesheet" type="text/css" media="screen" href="stylesheets/stylesheet.css">

    <title>Homb</title>
  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          <a id="forkme_banner" href="https://github.com/maxhutch/homb">View on GitHub</a>

          <h1 id="project_title">Homb</h1>
          <h2 id="project_tagline">Hybrid OpenMP MPI Benchmark</h2>

            <section id="downloads">
              <a class="zip_download_link" href="https://github.com/maxhutch/homb/zipball/master">Download this project as a .zip file</a>
              <a class="tar_download_link" href="https://github.com/maxhutch/homb/tarball/master">Download this project as a tar.gz file</a>
            </section>
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <h1>
<a name="hybrid-openmp-mpi-benchmark" class="anchor" href="#hybrid-openmp-mpi-benchmark"><span class="octicon octicon-link"></span></a>Hybrid OpenMP MPI Benchmark</h1>

<p>HOMB is the Hybrid OpenMP MPI Benchmark for testing hybrid codes on
multicore and SMP systems.  The code is an optimized Laplace Solver
on a 2-D grid (Point Jacobi) with optional convergence test.</p>

<p>The code is designed to compare the performance of MPI, OpenMP, and
hybrid codes.  It will behave as a pure MPI code when the number of
threads per task is 1 and a pure OpenMP code when the number of 
tasks is 1.</p>

<p>There are two general tests corespoding to three types of output:</p>

<ul>
<li>  Summary tests provide the statstical performance of HOMB</li>
<li>  Times tests provide the raw runtimes of each task</li>
</ul><p>To understand how these tests can be used to draw conclusions, it 
is recommended that the user reads the code over or simply use
summary tests output (-s).  </p>

<p>NOTE: all tests run the same code and only differ in output.</p>

<h2>
<a name="job-files" class="anchor" href="#job-files"><span class="octicon octicon-link"></span></a>Job Files</h2>

<p>Job files that compile and run HOMB are avaliable on these machines:</p>

<pre><code>PSC's Altix 4700 (Pople or Salk)
PSC's Xeon Cluster (Muir)
NCSA's Altix 3700 (Cobalt)
TACC's Sun Constellation Linux Cluster (Ranger)
</code></pre>

<h2>
<a name="to-build" class="anchor" href="#to-build"><span class="octicon octicon-link"></span></a>To Build</h2>

<p>If there is a <code>Make.mach.&lt;machine&gt;</code> file for the desired machine:</p>

<pre><code>make MACHINE=&lt;machine&gt;
</code></pre>

<p>If you want to add make support for your machine, copy a file for 
another machine and change the options.</p>

<p>--or--</p>

<p>Compile with MPI and OpenMP support and desired optimization flags.
Examples:</p>

<pre><code>icc -o homb.ex -O3 -ftz -ipo -openmp -lmpi src/homb.c
gcc -o homb.ex -O3 -fopenmp -lmpi src/homb.c
</code></pre>

<h2>
<a name="to-run" class="anchor" href="#to-run"><span class="octicon octicon-link"></span></a>To Run</h2>

<p>Run with MPI/OpenMP support and following args:</p>

<pre><code>-NC  val             Number of Columns in grid 
-NR  val             Number of Rows in grid    
-NRC val             Number of Rows and Columns in grid 
-NITER val           Number of iterations      
-[no_]barrier        Places MPI_Barrier at start of iterations
-[no_]reduce         Places MPI_Reduce at end of iterations
-s                   Summary to standard out
-v                   Verbose to standard out
-nh                  No header in standard out
-vf val              File name for verbose file output
-tf val              File name for times matrix file output
-pc                  Print Context (settings) to standard out
</code></pre>

<h2>
<a name="examples" class="anchor" href="#examples"><span class="octicon octicon-link"></span></a>Examples</h2>

<pre><code>mpirun -np 4 omplace -nt 4 ./homb.ex -NRC \
    -NITER 25 -vf homb-4-4-32768.out -s
</code></pre>

<p>Run on 16 cores with 4 tasks and 4 threads per task over
25 iterations with 512MB/core memory usage.  Outputs
all data to file and a summary to Standard Out.</p>

<pre><code>mpirun -np 4 ./homb.ex -NR 16384 -NC 16384 -NITER 10 -v
</code></pre>

<p>Run on 4 cores with pure MPI over 10 iterations with
512MB/core mem usage.  Outputs all data to stdout.</p>

<h2>
<a name="utilities" class="anchor" href="#utilities"><span class="octicon octicon-link"></span></a>Utilities</h2>

<h4>
<a name="times_to_histpy" class="anchor" href="#times_to_histpy"><span class="octicon octicon-link"></span></a><a href="utils/times_to_hist.py">times_to_hist.py</a>
</h4>

<p>This script takes a times output file from HOMB and produces a
histogram of the runtimes, allowing the user to analyze the 
distribution/variablility of the performance of the machine.</p>

<h4>
<a name="times_to_chainpy" class="anchor" href="#times_to_chainpy"><span class="octicon octicon-link"></span></a><a href="utils/times_to_chain.py">times_to_chain.py</a>
</h4>

<p>This script takes a times output file from HOMB and produces an
animation of exclusive runtimes of the individual PEs through time, 
allowing the user to see the relationships/coupling of the PEs.
The ouput can be thought of as the PE's chained together oscillating.
For example, on can deduce the general form of MPI_Reduce. </p>

<h4>
<a name="times_to_racepy" class="anchor" href="#times_to_racepy"><span class="octicon octicon-link"></span></a><a href="utils/times_to_race.py">times_to_race.py</a>
</h4>

<p>This script takes a times output file from HOMB and produces an
animation of inclusive runtimes of the individual PEs through time, 
allowing the user to see the relationships/coupling of the PEs.
The output can be thought of as the PE's racing (but low is good).
For example, on can deduce the general form of MPI_Reduce. </p>

<h4>
<a name="times_to_covarpy" class="anchor" href="#times_to_covarpy"><span class="octicon octicon-link"></span></a><a href="utils/times_to_covar.py">times_to_covar.py</a>
</h4>

<p>This script takes a times output file from HOMB and produces a
matrix of the covariances of ranks to one another.  This is useful
for finding the relative coupling of tasks.</p>

<h4>
<a name="sweep_to_graphpy" class="anchor" href="#sweep_to_graphpy"><span class="octicon octicon-link"></span></a><a href="utils/sweep_to_graph.py">sweep_to_graph.py</a>
</h4>

<p>This script takes a set of summary lines form multiple mulit-core 
runs and one single core run (of the same problem size) to plot
peak, average, and minimum performance of various MPI/OpenMP ratios</p>

<h4>
<a name="sweeps_to_scalespy" class="anchor" href="#sweeps_to_scalespy"><span class="octicon octicon-link"></span></a><a href="utils/sweeps_to_scales.py">sweeps_to_scales.py</a>
</h4>

<p>This script takes a set of summary lines form multiple mulit-core 
on multiple number of total cores (sweeps) and plots the peak, mean,
and worst performance to show weak-scaling.</p>

<h2>
<a name="more-info" class="anchor" href="#more-info"><span class="octicon octicon-link"></span></a>More info</h2>

<p>Max Hutchinson (<a href="mailto:mhutchin@psc.edu">mhutchin@psc.edu</a>).</p>

<p>Creator: Max Hutchinson (<a href="mailto:mhutchin@psc.edu">mhutchin@psc.edu</a>)</p>

<p>Proto-code written by: Uriel Jaroslawski, Jon Iturralde</p>
      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        <p class="copyright">Homb maintained by <a href="https://github.com/maxhutch">maxhutch</a></p>
        <p>Published with <a href="http://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    

  </body>
</html>
